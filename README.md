# Ingredient-Based Recipe Generator

This project takes a list of ingredients from the user, sends them to a backend service, and returns a recipe generated by a large language model. The backend uses Hugging Face Inference API with the `gemma-2-2b-it` model to produce a structured, markdown-formatted recipe.

## What the Project Does

1. User enters ingredients.
2. Frontend sends those ingredients to the backend.
3. Backend converts the list into a prompt.
4. Hugging Face's chat completion endpoint generates a recipe.
5. The generated recipe is returned to the frontend for display.

The backend does **not** validate ingredients, filter output, or enforce recipe structure beyond the system prompt. If the model fails, the backend throws the error.

## Backend Logic

The backend:
- Uses `@huggingface/inference` to call the `chatCompletion` API.
- Merges all ingredients into a comma-separated string.
- Sends a system message instructing the model to produce a markdown recipe.
- Requests up to 512 tokens of output.

### Core Function

```js
export async function getRecipeFromMistral(ingredientsArr) {
  const ingredientsString = ingredientsArr.join(", ");

  const response = await client.chatCompletion({
    model: "google/gemma-2-2b-it",
    messages: [
      { role: "system", content: SYSTEM_PROMPT },
      {
        role: "user",
        content: `I have ${ingredientsString}. Please give me a recipe you'd recommend I make!`,
      },
    ],
    max_tokens: 512,
  });

  return response.choices[0].message.content;
}
```
### API Behavior
- **Input:** List of ingredient strings.
- **Output:** Markdown-formatted recipe.
- **Failure Mode:** Logs the error, prints status/body if available, and re-throws.

### Requirements
- Node.js environment
- Hugging Face access token exposed as VITE_HF_ACCESS_TOKEN
- Valid model availability (google/gemma-2-2b-it)

### Setup
1. Install dependencies.
2. Create an environment variable:
```ini
VITE_HF_ACCESS_TOKEN=your_hf_token_here
```
3. Start the backend.
4. Frontend sends ingredient arrays to the backend endpoint that wraps getRecipeFromMistral.

### NOTES
- The model may introduce extra ingredients; this is intentional.
- Responses depend heavily on model behaviorâ€”do not assume consistency.
- If you need predictable formatting, add post-processing or enforce stricter prompts.

### Future Improvements
- Input validation before hitting the model.
- Optional controls (diet, region, complexity).
- Structured JSON recipe output instead of free-form markdown.
- Retry logic for flaky inference endpoints.